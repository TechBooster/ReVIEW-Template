= 目的から考えよう
===[column]
@<kw>{POINT}
 1. AIはデータが材料である。データがなければAIを作ることはできない。
 2. AIを作るために必要なデータは、意識的に収集しなければ用意することが難しい。
 3. だからこそ、開発を始める前に必要なデータを明確にする必要がある。
 4. 必要なデータを知るために、どんなAIを作るのか（目的）を決める必要がある。
===[/column]
== AIは「手段」であり「目的」ではない
@<kw>{「目的から考える」}ことの大切さは、AIアプリ開発以外においても共通だと思います。
しかし、データを活用するAIアプリは一般的なWebアプリよりも、最初の目的設定が特に重要です。
なぜかと言うと、AIアプリ開発は、そのモデル構築に必要なデータを集めるところから作業が始まるからです。

データを集め、分析する段階から@<kw>{「実現したいこと（目的）があって、その実現手段として機械学習（技術）が存在する」}
という因果を忘れてはいけません。目的がなければ、どんなAIアプリ開発も必ず迷子になります。

夢を感じる最新技術はすぐにでも使ってみたいものです。しかし、それが課題を解決するための最適な技術なのかをしっかりと検討する必要があります。

また、AIは基本的に@<kw>{開発にも運用にもコストがかかります。}
機械学習を用いてデータ分析するためには大量なデータが必要であり、そのために、データを保管できる十分なデータベースが必要だからです。
さらに、構築したモデルの精度は最初から良い数値が出るのは稀です。
つまり、モデル構築に使用したアルゴリズムは、継続的に改善しつづける必要があるということです。

だからこそ、開発中に手戻りが起こらないように、データ分析の作業が短縮できるように、
目的に対して一貫性のある無駄のない設計をおこなう必要があるのです。

== 鍵を握るのは@<kw>{目的(変数)に必要なデータセット}である
AIアプリ開発で必要なのは、何よりもまず@<kw>{目的(変数)に必要なデータセット}です。
日本の大企業がどれだけビッグデータを持っていても、データ活用が上手く進まないのはいくつかの理由があります。
その理由の1つは、蓄積されたデータが目的のために集められたデータではないため有効活用できないからです。

@<kw>{ただデータがあるだけでは、あなたの作りたいAIアプリを開発ことはできません。}これは紛れもない事実です。

== AI化までのステップ
AIを作るためには1つずつステップを踏んでいく必要があります。AI開発に王道なしです。
私はデータサイエンティストとして、クライアントがどのステップの段階にいるのかを見極めることを大切にしています。
私は図のようなピラミッド構造でAIプロジェクトを進めています。

== 目的設定
どんなAIを作りたいのか？つまり、どんなモデルを構築するのかを決めます。
目的は複数あることが多いので、@<kw>{1つの目的に、1つのモデルに整理し、1モデルごとに仮説を設定}します。
一般に、目的をモデルに落とし込む作業は機械学習の知識がなければ難しいでしょう。
ですから、このステップはデータサイエンティストに求められている大切な役割だと考えています。

=== 目的をモデルに落とし込んでみよう
目的をモデルに落とし込む方法は簡単です。
目的をどうするかを4つの選択肢から選んで整理するだけです。

4つの選択肢は
 1. 回帰：データを元に目的を説明する
 2. 分類：データを元に目的をAとBに分ける
 3. 推定：データを元に目的に与えている影響が何かを知る
 4. 予測：新しいデータから目的に関する未来の結果を得る
 です。

例えば、「私の理想の結婚相手と出逢うこと」が目的だとします。
このとき大切なことは「私の理想の結婚相手を@<kw>{どのように判断}したいのか」を1つに絞ってみることです。
「私の理想の結婚相手を@<kw>{予測}するモデル」「私の理想の結婚相手を@<kw>{分類}するモデル」は似て非なるモデルです。
分析のアプローチ方法が異なるのです。

図のようなシートで目的をモデルに変換してみましょう。
何となく実現したい目的が、具体的になってきた気がしませんか？

=== 仮説を立て、データを当てはめてみよう
仮説とは、何らかの現象や法則性を説明する判断内容・条件のことを指します。
これも具体的に考えてみましょう。

あなたが結婚相手に求める条件は何でしょうか。2つ以上条件を考えてみてください。

 （考えてから読み進めてください）

 外見？収入？性格？いろいろあるでしょう。
 きっとみなさんは2つ以上簡単に条件をあげることができたのではないでしょうか。
 私はこれが目的に対して仮説を立てる力の根源だと思っています。

 あとはこれを少しだけ客観的に想像してみるだけです。
 「理想の結婚相手には年収1,000万以上が条件である私」は、
 メジャーなのかマジョリティなのかを想像してみましょう。

仮説は自分の価値観や体験を、みんなはどうなのだろうか？と想像して
少しだけ一般化して整理したものだと考えています。
あとはこの条件を知るためには、どのようなデータが必要なのかを図のシートで整理してみましょう。

ここまでで、目的に対して必要なデータは何なのか？を整理することができました。
どうでしょう？あなたが作りたいモデルを作るために十分なデータは揃っていましたか？

難しく考えてしまうと、目的に必要なデータが何なのかわからなくなってしまいます。
@<kw>{目的に対して仮説を立てることができたらなら、あなたはもう立派なデータサイエンティスト}です。

特にAI化というのは、@<kw>{今まで人がやっていた判断基準を数値化・分類化する}ことが大きなテーマです。
その属人的な業務がいくつの要素で構成されており、それぞれの要素は目的に対してどのくらい影響しているか？
という仮説を立てて進めていきます。

理想の結婚相手の条件を挙げることができるあなたは、もうすでにこれができるのです。
このような感じで、AI化までのステップを理解していきましょう。

=== データ収集基盤の構築
目的を明確にすると、既存のデータでは不足していることが少なくありません。必要なデータが不足している場合は、データ収集基盤を構築します。

私は既存のデータでできることをやろうとするよりも、一層、精緻なデータ収集スキームを設計することをおすすめしています。
理由は、データ収集するタイミングを考えることは、属人的な業務のノウハウがどこに隠れているのかを深く理解することができるからです。

データ収集基盤構築は、アンケートでもいいと思います。この作業はデータとビジネスの距離感を近づけるとても大切なプロセスです。
テーブル定義書岳を見ていてもビジネスのことや本当の課題は理解できないし、データを見ずにビジネス論を語っていても何も進まないのです。

=== 集計／count
データが集まってきたらやるべきことは、分析ではなく@<kw>{集計}です。このとき大切なことは、改めてデータテーブルの列名（column）の意味を把握することと、indexに格納される値の種類と意味をしっかりと理解することです。

この基本的な集計作業をしっかりと行えば、目的設計で行った仮説が合っていたのか、外れていたのかをおおよそ当てることができます。分析しなくても、仮説の打率はおおよそ予測できます。だってもともと人がやっていたことを統計と機械の力を借りて型（モデル）にするだけですからね。
=== 中間テーブル作成
中間テーブルとは、そのデータを機械学習アルゴリズムで計算すれば、モデルが作れる整形済データセットのことを指します。目的が明確であれば、中間テーブルを作ることは簡単です。

目的設計の段階で中間テーブルの列名（収集したいデータの項目）は明確になっているはずです。私はここが@<kw>{データ分析を最短でデプロイするキーとなるポイント}だと信じています。

=== 機械学習を活用したデータ分析
みなさんがイメージしているデータ分析です。データ量を確認し、欠損の有無や欠損値を補完します。
その後、交差検証（要確認）などを行い、中間テーブルをより洗練されたものに進化させていきます。

個人的には、ここがデータサイエンティストとしての醍醐味なような気もしますが、この部分は最も機械による自動化が進んでいると思っています。
ですので本書でも詳しく説明する予定はありません。

大事なのは、ヒトであるわたしたちでしかできない目的設計とそれを具体化させた中間テーブルの設計です。
アカデミックである場合は別ですが、ここに時間をかけないことがデプロイまでの時間を短縮化させるポイントです。

=== モデル構築
実際にAIのアルゴリズムの元となるアルゴリズムを構築します。
この部分も機械が最も精度の高いアルゴリズムを選定してくれるので、あまり時間をかけずに進めていきます。

もちろん、モデル選定後にそのモデル自体の予測／分類精度を向上させることは大切です。この精度が低ければ、ただの人工無脳になってしまいますからね（笑）
一方で、デプロイ後にも改善できることも理解して欲しいです。目の前にカタチになったプロダクトがあったほうが進捗を感じます。

今回も精度改善の優先順位は少し下げて、次のステップへ進んでいきます。

=== システム組み込み（AI化）
AIアプリの定義はいろいろあると思いますが、今回はモデルをAPI化することによって既存のアプリケーション内のシステムに組み込むこととします。

モデルをシステムに組み込めるカタチで吐き出し、APIを作成していきましょう。ここまで走りきってからモデル改善にこだわったほうが、おそらく息が長く続きます。
ここまで進めばAIアプリの完成です。データ分析をデプロイできました。

== 新米データサイエンティストに必要な要素
 ここまでAI化までのプロセスを説明してきましたが、データサイエンティストのやるべきことって多くね？って思いませんか？（私は世の中は求め過ぎだ！と思います笑）
 本当にセクシーなデータサイエンティストは、アメリカの一流大学で数学と統計学、機械学習にはじまるテクノロジーの知識とプログラミングを勉強してシリコンバレーで働いている高給人材です。（妄想）

 統計学も機械学習もプログラミングも最近勉強したばかりの新米データサイエンティストの強みは@<kw>{最速の分析→デプロイテクニックの習得}ではないでしょうか？
 彼らよりも先に@<kw>{分析結果を個人でデプロイする力}さえ手に入れればいい。

=== 実務を通じて求められたデータサイエンティストの役割
 データサイエンティストは職種としては名前が定着してきたものの、役割が企業によって曖昧な気がしています。例えば、「機械学習を使って分析をするヒト」つぎに、「機械学習の仕組みをシステム化できるヒト」、はたまた「データベースを作ることができるヒト」
 でもこれを職種に直すと、「データアナリスト」「機械学習エンジニア」「データベースエンジニア」だと思っています。それが私の現時点での答えです。

 それでは、データサイエンティストの求められている役割って何なのかというと、@<kw>{「目的を明確にし、AI化できるマスタデータを設計できるヒト」}なのではないかな？と思っています。
 だから私は学生の頃から数学・統計を学んでいたり、院まで進んで機械学習を学ばなければならないと過度に勉強しすぎる必要はないし、逆に危険だと考えています。

 社会にデータからわかることを還元する際に、アカデミックな要素はそれほど求められていないからです。目的を数式に直す力。仮説でもいいから式を作れるヒト。それが新米データサイエンティストの役割なのではないでしょうか。

=== 最速デプロイのための目的整理シート

【ここに図が入ります】

=== 目的整理シートの使い方
