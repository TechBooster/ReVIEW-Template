= 目的から考えよう
===[column]
@<kw>{POINT}
 1. AIはデータが材料である。データがなければAIを作ることはできない。
 2. AIを作るために必要なデータは、意識的に収集しなければ用意することが難しい。
 3. だからこそ、開発を始める前に必要なデータを明確にする必要がある。
 4. 必要なデータを知るために、どんなAIを作るのか（目的）を決める必要がある。
===[/column]
== AIは「手段」であり「目的」ではない
@<kw>{「目的から考える」}ことの大切さは、AIアプリ開発以外においても共通だと思います。
しかし、データを活用するAIアプリは一般的なWebアプリよりも、最初の目的設定が特に重要です。
なぜかと言うと、AIアプリ開発は、そのモデル構築に必要なデータを集めるところから作業が始まるからです。

データを集め、分析する段階から@<kw>{「実現したいこと（目的）があって、その実現手段として機械学習（技術）が存在する」}
という因果を忘れてはいけません。目的がなければ、どんなAIアプリ開発も必ず迷子になります。

夢を感じる最新技術はすぐにでも使ってみたいものです。しかし、それが課題を解決するための最適な技術なのかをしっかりと検討する必要があります。

また、AIは基本的に@<kw>{開発にも運用にもコストがかかります。}
機械学習を用いてデータ分析するためには大量なデータが必要であり、そのために、データを保管できる十分なデータベースが必要だからです。
さらに、構築したモデルの精度は最初から良い数値が出るのは稀です。
つまり、モデル構築に使用したアルゴリズムは、継続的に改善しつづける必要があるということです。

だからこそ、開発中に手戻りが起こらないように、データ分析の作業が短縮できるように、
目的に対して一貫性のある無駄のない設計をおこなう必要があるのです。

== 鍵を握るのは@<kw>{目的(変数)に必要なデータセット}である
AIアプリ開発で必要なのは、何よりもまず@<kw>{目的(変数)に必要なデータセット}です。
日本の大企業がどれだけビッグデータを持っていても、データ活用が上手く進まないのはいくつかの理由があります。
その理由の1つは、蓄積されたデータが目的のために集められたデータではないため有効活用できないからです。

@<kw>{ただデータがあるだけでは、あなたの作りたいAIアプリを開発ことはできません。}これは紛れもない事実です。

== AI化までのステップ
AIを作るためには1つずつステップを踏んでいく必要があります。AI開発に王道なしです。
私はデータサイエンティストとして、クライアントがどのステップの段階にいるのかを見極めることを大切にしています。
私は図のようなピラミッド構造でAIプロジェクトを進めています。

== 目的設定
どんなAIを作りたいのか？つまり、どんなモデルを構築するのかを決めます。
目的は複数あることが多いので、@<kw>{1つの目的に、1つのモデルに整理し、1モデルごとに仮説を設定}します。
一般に、目的をモデルに落とし込む作業は機械学習の知識がなければ難しいでしょう。
ですから、このステップはデータサイエンティストに求められている大切な役割だと考えています。

=== 目的をモデルに落とし込んでみよう
目的をモデルに落とし込む方法は簡単です。
目的をどうするかを4つの選択肢から選んで整理するだけです。

4つの選択肢は
 1. 回帰：データを元に目的を説明する
 2. 分類：データを元に目的をAとBに分ける
 3. 推定：データを元に目的に与えている影響が何かを知る
 4. 予測：新しいデータから目的に関する未来の結果を得る
 です。

例えば、「私の理想の結婚相手と出逢うこと」が目的だとします。
このとき大切なことは「私の理想の結婚相手を@<kw>{どのように判断}したいのか」を1つに絞ってみることです。
「私の理想の結婚相手を@<kw>{予測}するモデル」「私の理想の結婚相手を@<kw>{分類}するモデル」は似て非なるモデルです。
分析のアプローチ方法が異なるのです。

図のようなシートで目的をモデルに変換してみましょう。
何となく実現したい目的が、具体的になってきた気がしませんか？

【最速デプロイのための目的整理シート】

=== 仮説を立て、データを当てはめてみよう
仮説とは、何らかの現象や法則性を説明する判断内容・条件のことを指します。
これも具体的に考えてみましょう。

あなたが結婚相手に求める条件は何でしょうか。2つ以上条件を考えてみてください。

 （考えてから読み進めてください）

 外見？収入？性格？いろいろあるでしょう。
 きっとみなさんは2つ以上簡単に条件をあげることができたのではないでしょうか。
 私はこれが目的に対して仮説を立てる力の根源だと思っています。

 あとはこれを少しだけ客観的に想像してみるだけです。
 「理想の結婚相手には年収1,000万以上が条件である私」は、
 メジャーなのかマジョリティなのかを想像してみましょう。

仮説は自分の価値観や体験を、みんなはどうなのだろうか？と想像して
少しだけ一般化して整理したものだと考えています。
あとはこの条件を知るためには、どのようなデータが必要なのかを図のシートで整理してみましょう。

【最速デプロイのための仮説のデータ化シート】

ここまでで、目的に対して必要なデータは何なのか？を整理することができました。
どうでしょう？あなたが作りたいモデルを作るために十分なデータは揃っていましたか？

難しく考えてしまうと、目的に必要なデータが何なのかわからなくなってしまいます。
@<kw>{目的に対して仮説を立てることができたらなら、あなたはもう立派なデータサイエンティスト}です。

特にAI化というのは、@<kw>{今まで人がやっていた判断基準を数値化・分類化する}ことが大きなテーマです。
その属人的な業務がいくつの要素で構成されており、それぞれの要素は目的に対してどのくらい影響しているか？
という仮説を立てて進めていきます。

理想の結婚相手の条件を挙げることができるあなたは、もうすでにこれができるのです。
このような感じで、AI化までのステップを理解していきましょう。

== データ収集基盤の構築
目的と仮説が明確になると、目的を実現するためのデータが不足しているかどうかを把握することができます。
必要なデータが不足している場合は、インターネット上に公開されているデータを活用したり、あるいはデータ収集基盤を構築します。

もちろん企業の場合は、既存データを活用して取り組もうとするケースが多いでしょう。
しかし、私は思い切って必要なデータを確実に蓄積できる@<kw>{データ収集基盤}を作ることをおすすめします。

理由は2つあります。まず、データ収集するタイミングを考えることは、属人的な業務のノウハウがどこに隠れているのかを深く理解することに繋がるからです。
つぎに、既存データを目的に使えるようにデータを整形する作業は想像以上に時間がかかって効率が悪いからです。

データ収集方法を考える作業は@<kw>{データとビジネスの距離感を近づけるとても大切なプロセス}です。
既存データのテーブル定義書だけを見ていても、実現したい目的に対しての本当の課題は理解することは難しいです。
逆に、データ収集方法を考えずに、ビジネスのお話だけを語っていてもAI開発は何も進まないのです。

== 数え上げ（count）／集計
いよいよデータが集まってきたらやるべきことは、@<kw>{分析ではなく集計}です。
このとき大切なことは、@<kw>{データラベル, 列名・カラム名}の意味を把握することと、indexに格納される値の種類と意味をしっかりと理解することです。
当たり前ですが、「◯◯日から◯◯日の間に何がいくつあるのか」といった「数えて、集める」という基本を徹底できずに有効なAIを作れるとは思いません。

一般的には、モデリング前に行う基礎分析は、@<kw>{探索的分析}を指します。
探索的分析はモデリングの精度を上げるためには非常に有効な分析プロセスです。
しかし、数字に対する肌感覚を掴むため、まずはこの基本的な集計作業をおこなうことを徹底するべきです。

また、これだけでも十分にデータによる意思決定（判断）ができます。
ですから、本書では無料だから個人でも使える「Googleデータポータル」というBIツールでの
ダッシュボード作成方法も付録で紹介しています。　

== 中間テーブル作成
中間テーブルは関連テーブルとも呼ばれ、多対多のデータの関係を表すテーブルを意味しています。
データベースを設計したことがない人にとっては専門的で難しいと思うので、詳しい説明は割愛します。

ざっくり言えば、@<kw>{分析やモデリングで必要な列と行だけを抽出したテーブル}だと理解していただければ十分です。
最初のステップで取り組んだ目的設定と仮説が明確であれば、中間テーブルを作ることは簡単です。
目的設定の段階で中間テーブルの列名（収集したいデータの項目）に目星がついているはずだからです。
私はここが@<kw>{データ分析を最短でデプロイするキーポイント}だと考えています。

大企業が保有しているデータを活用してAIを作るときは、どこにどんなデータが存在しているのかを把握するにも一苦労です。
特に、それぞれのデータテーブルがそれぞれの目的をもって存在しているため、中間テーブルを作ること自体が困難な場合も少なくありません。

実際、データ分析に至るまでのステップに時間がかかります。
ですから、今回のAIアプリでは、このステップから始められるようにオープンデータを活用して
サンプルアプリを開発していきます。

== データ分析
みなさんがイメージしている機械学習を活用したデータ分析のステップです。
これまでのステップでデータの意味を理解しているはずです。

データ分析の中にも、いくつかのステップがあります。
このステップは、中間テーブル作成の時点で洗練されたテーブルが作られていれば、
殆どの作業をスキップすることができます。

=== 欠損値の確認
まずはデータ量・欠損値の有無を確認します。
データ分析するのに十分な値があるかどうかを確認していく作業です。
欠損がある場合は、欠損値を補完します。

=== 探索的分析



その後、交差検証（要確認）などを行い、中間テーブルをより洗練されたものに進化させていきます。

個人的には、ここがデータサイエンティストとしての醍醐味なような気もしますが、この部分は最も機械による自動化が進んでいると思っています。
ですので本書でも詳しく説明する予定はありません。

大事なのは、ヒトであるわたしたちでしかできない目的設計とそれを具体化させた中間テーブルの設計です。
アカデミックである場合は別ですが、ここに時間をかけないことがデプロイまでの時間を短縮化させるポイントです。

== モデル構築（モデリング）
実際にAIのアルゴリズムの元となるアルゴリズムを構築します。
この部分も機械が最も精度の高いアルゴリズムを選定してくれるので、あまり時間をかけずに進めていきます。

もちろん、モデル選定後にそのモデル自体の予測／分類精度を向上させることは大切です。この精度が低ければ、ただの人工無脳になってしまいますからね（笑）
一方で、デプロイ後にも改善できることも理解して欲しいです。目の前にカタチになったプロダクトがあったほうが進捗を感じます。

今回も精度改善の優先順位は少し下げて、次のステップへ進んでいきます。

=== システム組み込み（AI化）
AIアプリの定義はいろいろあると思いますが、今回はモデルをAPI化することによって既存のアプリケーション内のシステムに組み込むこととします。

モデルをシステムに組み込めるカタチで吐き出し、APIを作成していきましょう。ここまで走りきってからモデル改善にこだわったほうが、おそらく息が長く続きます。
ここまで進めばAIアプリの完成です。データ分析をデプロイできました。

== 新米データサイエンティストに必要な要素
 ここまでAI化までのプロセスを説明してきましたが、データサイエンティストのやるべきことって多くね？って思いませんか？（私は世の中は求め過ぎだ！と思います笑）
 本当にセクシーなデータサイエンティストは、アメリカの一流大学で数学と統計学、機械学習にはじまるテクノロジーの知識とプログラミングを勉強してシリコンバレーで働いている高給人材です。（妄想）

 統計学も機械学習もプログラミングも最近勉強したばかりの新米データサイエンティストの強みは@<kw>{最速の分析→デプロイテクニックの習得}ではないでしょうか？
 彼らよりも先に@<kw>{分析結果を個人でデプロイする力}さえ手に入れればいい。

=== 実務を通じて求められたデータサイエンティストの役割
 データサイエンティストは職種としては名前が定着してきたものの、役割が企業によって曖昧な気がしています。例えば、「機械学習を使って分析をするヒト」つぎに、「機械学習の仕組みをシステム化できるヒト」、はたまた「データベースを作ることができるヒト」
 でもこれを職種に直すと、「データアナリスト」「機械学習エンジニア」「データベースエンジニア」だと思っています。それが私の現時点での答えです。

 それでは、データサイエンティストの求められている役割って何なのかというと、@<kw>{「目的を明確にし、AI化できるマスタデータを設計できるヒト」}なのではないかな？と思っています。
 だから私は学生の頃から数学・統計を学んでいたり、院まで進んで機械学習を学ばなければならないと過度に勉強しすぎる必要はないし、逆に危険だと考えています。

 社会にデータからわかることを還元する際に、アカデミックな要素はそれほど求められていないからです。目的を数式に直す力。仮説でもいいから式を作れるヒト。それが新米データサイエンティストの役割なのではないでしょうか。
